{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Helmholz_loss(u,a,boundary,omega=5*torch.pi/2,p=1,D=1):\n",
    "    '''Calculates the PDE loss for the Poisson equation\n",
    "    Input: u  Output of network, Shape = (Batch_size,Grid_size,Grid_size)\n",
    "           a  Input  of network, Shape = (Batch_size,Grid_size,Grid_size)\n",
    "           boundary Boundary conditon of u, float\n",
    "           omega wave number\n",
    "           p  Do we use L1 or L2 errors? Default: L1\n",
    "           D  Period of Domain\n",
    "    Warning: Input f and Output u should not be normalized!'''\n",
    "\n",
    "    Laplace_u=Laplace(u,D=D)\n",
    "\n",
    "    if p == 1:\n",
    "      loss = torch.nn.L1Loss()\n",
    "    elif p == 2:\n",
    "      loss = torch.nn.MSELoss()\n",
    "\n",
    "    loss_pde=loss(Laplace_u,-omega**2*a**2*u)\n",
    "\n",
    "    #Add boundary loss: u=0 on boundary(Domain)\n",
    "    boundary_lossx_0=loss(u[:,0,:], boundary*torch.ones_like(u[:,0,:]))\n",
    "    boundary_lossx_D=loss(u[:,-1,:],boundary*torch.ones_like(u[:,-1,:]))\n",
    "    boundary_lossy_D=loss(u[:,:,-1],boundary*torch.ones_like(u[:,:,1]))\n",
    "    boundary_lossy_0=loss(u[:,:,0], boundary*torch.ones_like(u[:,:,0]))\n",
    "    boundary_loss=0.25*(boundary_lossx_0+boundary_lossy_0+boundary_lossx_D+boundary_lossy_D)\n",
    "    loss=0.5*(loss_pde+boundary_loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "def Poisson_pde_loss(u,f,p,D=2):\n",
    "    '''Calculates the pde loss for the Poisson equation'''\n",
    "    s=u.size(-1)\n",
    "    \n",
    "    u_hat=torch.fft.fft2(u,dim=[-2,-1])\n",
    "    assert (u.device==u_hat.device)\n",
    "    k_max=s//2\n",
    "    \n",
    "    k_x = torch.cat((torch.arange(start=0, end=k_max, step=1, device=u.device),\n",
    "                     torch.arange(start=-k_max, end=0, step=1, device=u.device)), 0).reshape(s, 1).repeat(1, s).reshape(1,s,s)\n",
    "    k_y = torch.cat((torch.arange(start=0, end=k_max, step=1, device=u.device),\n",
    "                     torch.arange(start=-k_max, end=0, step=1, device=u.device)), 0).reshape(1, s).repeat(s, 1).reshape(1,s,s)\n",
    "    \n",
    "    \n",
    "    Laplace_u_hat =-4*(torch.pi/D)**2*(k_x**2+k_y**2)*u_hat\n",
    "\n",
    "\n",
    "    Laplace_u=torch.fft.irfft2(Laplace_u_hat[:, :, :k_max + 1], dim=[-2, -1])\n",
    "\n",
    "    if p == 1:\n",
    "      loss = torch.nn.L1Loss()\n",
    "    elif p == 2:\n",
    "      loss = torch.nn.MSELoss()\n",
    "    \n",
    "    epsilon=1e-10\n",
    "\n",
    "    loss_pde = loss(-Laplace_u, f)/(loss(torch.zeros_like(f).to(u.device), f) + epsilon)\n",
    "    \n",
    "    return loss_pde, Laplace_u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0140)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Arno\\OneDrive - ETHZ\\Masterarbeit\\Code\\PrivateConvolutionalNeuralOperator\\Test.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arno/OneDrive%20-%20ETHZ/Masterarbeit/Code/PrivateConvolutionalNeuralOperator/Test.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(calculate_f(D\u001b[39m*\u001b[39m\u001b[39m0\u001b[39m\u001b[39m/\u001b[39ms,D\u001b[39m*\u001b[39m \u001b[39m0\u001b[39m\u001b[39m/\u001b[39ms, K, a, r)\u001b[39m-\u001b[39mcalculate_f(D\u001b[39m*\u001b[39m(s\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39ms,D\u001b[39m*\u001b[39m (s\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39ms, K, a, r))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arno/OneDrive%20-%20ETHZ/Masterarbeit/Code/PrivateConvolutionalNeuralOperator/Test.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m u\u001b[39m=\u001b[39mu\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Arno/OneDrive%20-%20ETHZ/Masterarbeit/Code/PrivateConvolutionalNeuralOperator/Test.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m loss1,loss2,Du,u_lap\u001b[39m=\u001b[39mPoisson_pde_loss(u\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m),f\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m),\u001b[39m1\u001b[39m,D\u001b[39m=\u001b[39mD)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arno/OneDrive%20-%20ETHZ/Masterarbeit/Code/PrivateConvolutionalNeuralOperator/Test.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(loss1,loss2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arno/OneDrive%20-%20ETHZ/Masterarbeit/Code/PrivateConvolutionalNeuralOperator/Test.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m y\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\Arno\\OneDrive - ETHZ\\Masterarbeit\\Code\\PrivateConvolutionalNeuralOperator\\Test.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arno/OneDrive%20-%20ETHZ/Masterarbeit/Code/PrivateConvolutionalNeuralOperator/Test.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m   loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arno/OneDrive%20-%20ETHZ/Masterarbeit/Code/PrivateConvolutionalNeuralOperator/Test.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m epsilon\u001b[39m=\u001b[39m\u001b[39m1e-10\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Arno/OneDrive%20-%20ETHZ/Masterarbeit/Code/PrivateConvolutionalNeuralOperator/Test.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss_pde \u001b[39m=\u001b[39m loss(\u001b[39m-\u001b[39mDu[:,:,\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], f[:,\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m/\u001b[39m (loss(torch\u001b[39m.\u001b[39mzeros_like(f)\u001b[39m.\u001b[39mto(u\u001b[39m.\u001b[39mdevice), f) \u001b[39m+\u001b[39m epsilon)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arno/OneDrive%20-%20ETHZ/Masterarbeit/Code/PrivateConvolutionalNeuralOperator/Test.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m loss_pde2 \u001b[39m=\u001b[39m loss(\u001b[39m-\u001b[39mu_lap, f)\u001b[39m/\u001b[39m(loss(torch\u001b[39m.\u001b[39mzeros_like(f)\u001b[39m.\u001b[39mto(u\u001b[39m.\u001b[39mdevice), f) \u001b[39m+\u001b[39m epsilon)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Arno/OneDrive%20-%20ETHZ/Masterarbeit/Code/PrivateConvolutionalNeuralOperator/Test.ipynb#W1sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss_pde, loss_pde2, \u001b[39m-\u001b[39mDu,\u001b[39m-\u001b[39mu_lap\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "#Check if loss function works correctly\n",
    "\n",
    "def calculate_f(x, y, K, a, r):\n",
    "    result = 0\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a[i])):\n",
    "            result += a[i][j] *  ((i)**2 + (j)**2)**0.5 * np.sin(np.pi * (i) * x) * np.sin(np.pi * (j) * y)\n",
    "    return (np.pi  / ( K**2))* result\n",
    "\n",
    "\n",
    "def calculate_u(x, y, K, a, r):\n",
    "    result = 0\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a[i])):\n",
    "            if (i**2 + j**2) == 0:\n",
    "                result += 0  \n",
    "            else:\n",
    "                result += a[i][j] * ((i)**2 + (j)**2)**(-1/2) * np.sin(np.pi * (i) * x) * np.sin(np.pi * (j) * y)\n",
    "    return (1 / (np.pi * K**2)) * result\n",
    "\n",
    "s=64\n",
    "f=torch.zeros((1,1,s,s))\n",
    "u=torch.zeros((1,1,s,s))\n",
    "error=torch.rand((1,1,s,s))*1e-5\n",
    "#Really senitive to error\n",
    "\n",
    "K=3\n",
    "a = 2 * torch.rand((K,K)) - 1\n",
    "r=0.5\n",
    "D=2\n",
    "for x in range(0,s):\n",
    "    for y in range(0,s):\n",
    "        f[0,0,x,y]=calculate_f(D*x/s,D* y/s, K, a, r)\n",
    "        u[0,0,x,y]=calculate_u(D*x/s,D* y/s, K, a, r)\n",
    "\n",
    "print(calculate_f(D*0/s,D* 0/s, K, a, r)-calculate_f(D*(s+1)/s,D* (s+1)/s, K, a, r))\n",
    "u=u\n",
    "\n",
    "loss,u_lap=Poisson_pde_loss(u.squeeze(1),f.squeeze(1),1,D=D)\n",
    "\n",
    "print(loss)\n",
    "y=4\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(np.arange(0,s),u[0,0,:,y]-error[0,0,:,y],label='u_error')\n",
    "plt.plot(np.arange(0,s),u[0,0,:,y],label='u')\n",
    "plt.plot(np.arange(0,s),f[0,0,:,y],label='f')\n",
    "plt.plot(np.arange(0,s),u_lap[0,:,y],label='u_lab')\n",
    "plt.legend()\n",
    "\n",
    "print(u_lap[0,:,y]/f[0,0,:,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wave_pde_loss(u,u0,c=0.1,T=5,p=1):\n",
    "    '''Calculates the PDE loss for the Wave equation\n",
    "       Input:  u   Output of network, Shape = (Batch_size,Spatial_grid_size,Spatial_grid_size,Temporal_grid_size)\n",
    "               u0  Input  of network, Shape = (Batch_size,Spatial_grid_size,Spatial_grid_size)\n",
    "               p   Do we use L1 i.e (p==1) or L2 i.e (p==2) errors? Default: L1'''\n",
    "   \n",
    "    Temporal_grid_size=u.size(1)\n",
    "    Spatial_grid_size=u.size(-1)\n",
    "    #Calculate derivative using the fact FT(f')=2*pi*i*k*FT(f) (Where we use torch, Bogdan uses scypi)\n",
    "    u_hat=torch.fft.fft2(u,dim=[-2,-1])\n",
    "    print(u_hat.shape)\n",
    "\n",
    "    assert (u.device==u_hat.device) #Need to be same device, can only be checked on GPU\n",
    "\n",
    "    #Doesnt this make it a grid based loss? (No it shouldnt!)\n",
    "    k_max=Spatial_grid_size//2\n",
    "    dt=T/(Temporal_grid_size-1)\n",
    "    \n",
    "    k_x = torch.cat((torch.arange(start=0, end=k_max, step=1, device=u.device),\n",
    "                     torch.arange(start=-k_max, end=0, step=1, device=u.device)), 0).reshape(Spatial_grid_size, 1).repeat(1, Spatial_grid_size).reshape(1,1,Spatial_grid_size,Spatial_grid_size)\n",
    "    k_y = torch.cat((torch.arange(start=0, end=k_max, step=1, device=u.device),\n",
    "                     torch.arange(start=-k_max, end=0, step=1, device=u.device)), 0).reshape(1, Spatial_grid_size).repeat(Spatial_grid_size, 1).reshape(1,1,Spatial_grid_size,Spatial_grid_size)\n",
    "    \n",
    "    ux_hat  = 2j *torch.pi*k_x*u_hat\n",
    "    uxx_hat = 2j *torch.pi*k_x*ux_hat\n",
    "    \n",
    "    uy_hat  = 2j*torch.pi*k_y*u_hat\n",
    "    uyy_hat = 2j*torch.pi*k_y*uy_hat\n",
    "\n",
    "    \n",
    "    uxx = torch.fft.irfft2(uxx_hat[:, :,:, :k_max + 1],   dim=[-2, -1])\n",
    "    uyy = torch.fft.irfft2(uyy_hat[:, :,:, :k_max + 1],   dim=[-2, -1])\n",
    "\n",
    "    #Calculate Laplace-Operator for u\n",
    "    Du=(uyy+uxx)/4\n",
    "   \n",
    "    utt = (u[:, 2:,...] - 2.0*u[:, 1:-1,...] + u[:, :-2,...]) / (dt**2)\n",
    "\n",
    "    if p == 1:\n",
    "      loss = torch.nn.L1Loss()\n",
    "    elif p == 2:\n",
    "      loss = torch.nn.MSELoss()\n",
    "    \n",
    "    epsilon=1e-10\n",
    "\n",
    "    #Here I am not sure if I should use the relative loss\n",
    "    loss_pde = loss(Du[:, 1:-1,:],c**2* utt)\n",
    "      \n",
    "    print(loss_pde)\n",
    "    #Add relative boundary loss: u=0 on boundary(Domain)\n",
    "    inital_loss=loss(u0, u[:,0,...])/(loss(torch.zeros_like(u0).to(u0.device), u0)+epsilon)\n",
    "\n",
    "    return 0.5*(loss_pde+inital_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 32, 32])\n",
      "tensor(0.4671)\n",
      "tensor(0.2335)\n"
     ]
    }
   ],
   "source": [
    "def calculate_u0(x, y, K, a):\n",
    "    result = 0\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a[i])):\n",
    "            if (i**2 + j**2) == 0:\n",
    "                result += 0  \n",
    "            else:\n",
    "               result += a[i][j] * (i**2 + j**2)**(-1) * np.sin(np.pi * i * x) * np.sin(np.pi * j * y)\n",
    "    return (1 / (K**2))* result\n",
    "def calculate_u_wave(x, y, t, K, a,c):\n",
    "    result = 0\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a[i])):\n",
    "            if (i**2 + j**2) == 0:\n",
    "                result += 0  \n",
    "            else:\n",
    "                result += a[i][j] * (i**2 + j**2)**(-1) * np.sin(np.pi * i * x) * np.sin(np.pi * j * y)*np.cos(np.pi *c* np.sqrt(i**2+j**2) * t)\n",
    "    return (1 / (K**2))* result\n",
    "s=32\n",
    "T=10\n",
    "u0=torch.zeros((1,1,s,s))\n",
    "u=torch.zeros((1,T,s,s))\n",
    "error=torch.rand((1,1,s,s))*1e-10\n",
    "K=3\n",
    "a = 2 * torch.rand((K,K)) - 1\n",
    "c=0.1\n",
    "r=0.5\n",
    "for x in range(-s//2,s//2):\n",
    "    for y in range(-s//2,s//2):  \n",
    "        u0[0,0,x+s//2,y+s//2]=calculate_u0(x/s*2, y/s*2, K, a)\n",
    "        for t in range(T):\n",
    "          u[0,t,x+s//2,y+s//2]=calculate_u_wave(x/s*2, y/s*2,t/1000, K, a, c)\n",
    "#u=u+error\n",
    "\n",
    "loss=Wave_pde_loss(u,u0.squeeze(1),2)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5699e-07)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.7684e-07, 0.0000e+00, 0.0000e+00, 1.9073e-06, 2.3842e-07, 2.3842e-07,\n",
       "         1.9073e-06, 7.4506e-09, 5.9605e-08, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 3.8147e-06, 5.9605e-08, 2.3842e-07, 5.9605e-08,\n",
       "         3.8147e-06, 0.0000e+00, 1.9073e-06, 2.3283e-10],\n",
       "        [2.8610e-06, 9.5367e-07, 0.0000e+00, 2.3842e-07, 0.0000e+00, 1.9073e-06,\n",
       "         3.8147e-06, 5.7220e-06, 4.7684e-07, 4.7684e-07],\n",
       "        [1.9073e-06, 0.0000e+00, 3.8147e-06, 9.5367e-07, 4.6566e-10, 4.7684e-07,\n",
       "         3.8147e-06, 1.9073e-06, 9.5367e-07, 9.5367e-07],\n",
       "        [2.3842e-07, 9.5367e-07, 9.5367e-07, 1.9073e-06, 9.5367e-07, 7.6294e-06,\n",
       "         9.5367e-07, 2.3842e-07, 0.0000e+00, 1.1642e-10],\n",
       "        [7.6294e-06, 1.9073e-06, 3.8147e-06, 3.8147e-06, 5.7220e-06, 4.7684e-07,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9802e-08],\n",
       "        [0.0000e+00, 1.9073e-06, 2.9802e-08, 9.5367e-07, 1.9073e-06, 2.9802e-08,\n",
       "         1.9073e-06, 7.6294e-06, 5.9605e-08, 0.0000e+00],\n",
       "        [4.7684e-07, 1.1921e-07, 3.8147e-06, 3.8147e-06, 0.0000e+00, 1.1921e-07,\n",
       "         0.0000e+00, 0.0000e+00, 9.5367e-07, 0.0000e+00],\n",
       "        [0.0000e+00, 9.5367e-07, 2.3283e-10, 4.7684e-07, 5.9605e-08, 1.9073e-06,\n",
       "         3.8147e-06, 0.0000e+00, 3.8147e-06, 9.5367e-07],\n",
       "        [2.9802e-08, 9.5367e-07, 7.4506e-09, 1.4901e-08, 0.0000e+00, 0.0000e+00,\n",
       "         9.5367e-07, 1.1921e-07, 3.8147e-06, 9.5367e-07]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,4,4,2)\n",
    "s=4//2\n",
    "m=torch.fft.fft2(x,dim=[-3,-2])\n",
    "x_h=torch.fft.irfft2(m[:,:,:s+1,:],dim=[-3,-2])\n",
    "print(torch.norm(x-x_h))\n",
    "\n",
    "c=torch.rand(10,10)\n",
    "a=(2*torch.pi*c)**2\n",
    "b=(c)**2*(2*torch.pi)**2\n",
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0.weight, Requires Grad: False\n",
      "Parameter: 0.bias, Requires Grad: True\n",
      "Parameter: 2.weight, Requires Grad: True\n",
      "Parameter: 2.bias, Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a sample model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 2),\n",
    ")\n",
    "\n",
    "num_layers_to_freeze=0.25\n",
    "# Print the initial gradient status\n",
    "\n",
    "\n",
    "# Freeze specific layers or parameters\n",
    "for i, param in enumerate(model.parameters()):\n",
    "    if i < num_layers_to_freeze*len(list(model.parameters())):\n",
    "        param.requires_grad = False\n",
    " \n",
    "\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Print the updated gradient status\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project': 3, 'lift': 3, 'cont_conv_layers': 12, 'cont_conv_layers_invariant': 12, 'res_batch_norm_in': 6, 'batch_norm': 8, 'batch_norm_inv': 8, 'resnet_blocks': 180}\n",
      "Prefix: project, Number of Layers: 3\n",
      "Prefix: lift, Number of Layers: 3\n",
      "Prefix: cont_conv_layers, Number of Layers: 12\n",
      "Prefix: cont_conv_layers_invariant, Number of Layers: 12\n",
      "Prefix: res_batch_norm_in, Number of Layers: 6\n",
      "Prefix: batch_norm, Number of Layers: 8\n",
      "Prefix: batch_norm_inv, Number of Layers: 8\n",
      "Prefix: resnet_blocks, Number of Layers: 180\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Define your parameter names\n",
    "parameter_names = [\n",
    "\"project.bias\" ,\n",
    "\"project.convolution.weight\",\n",
    "\"project.convolution.bias\",\n",
    "\"lift.bias\",\n",
    "\"lift.convolution.weight\",\n",
    "\"lift.convolution.bias\",\n",
    "\"cont_conv_layers.0.bias\",\n",
    "\"cont_conv_layers.0.convolution.weight\",\n",
    "\"cont_conv_layers.0.convolution.bias\",\n",
    "\"cont_conv_layers.1.bias\",\n",
    "\"cont_conv_layers.1.convolution.weight\",\n",
    "\"cont_conv_layers.1.convolution.bias\",\n",
    "\"cont_conv_layers.2.bias\",\n",
    "\"cont_conv_layers.2.convolution.weight\",\n",
    "\"cont_conv_layers.2.convolution.bias\",\n",
    "\"cont_conv_layers.3.bias\",\n",
    "\"cont_conv_layers.3.convolution.weight\",\n",
    "\"cont_conv_layers.3.convolution.bias\",\n",
    "\"cont_conv_layers_invariant.0.bias\",\n",
    "\"cont_conv_layers_invariant.0.convolution.weight\",\n",
    "\"cont_conv_layers_invariant.0.convolution.bias\",\n",
    "\"cont_conv_layers_invariant.1.bias\",\n",
    "\"cont_conv_layers_invariant.1.convolution.weight\",\n",
    "\"cont_conv_layers_invariant.1.convolution.bias\",\n",
    "\"cont_conv_layers_invariant.2.bias\",\n",
    "\"cont_conv_layers_invariant.2.convolution.weight\",\n",
    "\"cont_conv_layers_invariant.2.convolution.bias\",\n",
    "\"cont_conv_layers_invariant.3.bias\",\n",
    "\"cont_conv_layers_invariant.3.convolution.weight\",\n",
    "\"cont_conv_layers_invariant.3.convolution.bias\",\n",
    "\"res_batch_norm_in.0.weight\",\n",
    "\"res_batch_norm_in.0.bias\",\n",
    "\"res_batch_norm_in.1.weight\",\n",
    "\"res_batch_norm_in.1.bias\",\n",
    "\"res_batch_norm_in.2.weight\",\n",
    "\"res_batch_norm_in.2.bias\",\n",
    "\"batch_norm.0.weight\",\n",
    "\"batch_norm.0.bias\",\n",
    "\"batch_norm.1.weight\",\n",
    "\"batch_norm.1.bias\",\n",
    "\"batch_norm.2.weight\",\n",
    "\"batch_norm.2.bias\",\n",
    "\"batch_norm.3.weight\",\n",
    "\"batch_norm.3.bias\",\n",
    "\"batch_norm_inv.0.weight\",\n",
    "\"batch_norm_inv.0.bias\",\n",
    "\"batch_norm_inv.1.weight\",\n",
    "\"batch_norm_inv.1.bias\",\n",
    "\"batch_norm_inv.2.weight\",\n",
    "\"batch_norm_inv.2.bias\",\n",
    "\"batch_norm_inv.3.weight\",\n",
    "\"batch_norm_inv.3.bias\",\n",
    "\"resnet_blocks.0.cont_conv.0.bias\",\n",
    "\"resnet_blocks.0.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.0.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.0.cont_conv.1.bias\",\n",
    "\"resnet_blocks.0.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.0.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.1.cont_conv.0.bias\",\n",
    "\"resnet_blocks.1.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.1.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.1.cont_conv.1.bias\",\n",
    "\"resnet_blocks.1.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.1.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.2.cont_conv.0.bias\",\n",
    "\"resnet_blocks.2.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.2.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.2.cont_conv.1.bias\",\n",
    "\"resnet_blocks.2.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.2.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.3.cont_conv.0.bias\",\n",
    "\"resnet_blocks.3.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.3.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.3.cont_conv.1.bias\",\n",
    "\"resnet_blocks.3.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.3.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.4.cont_conv.0.bias\",\n",
    "\"resnet_blocks.4.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.4.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.4.cont_conv.1.bias\",\n",
    "\"resnet_blocks.4.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.4.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.5.cont_conv.0.bias\",\n",
    "\"resnet_blocks.5.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.5.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.5.cont_conv.1.bias\",\n",
    "\"resnet_blocks.5.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.5.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.6.cont_conv.0.bias\",\n",
    "\"resnet_blocks.6.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.6.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.6.cont_conv.1.bias\",\n",
    "\"resnet_blocks.6.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.6.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.7.cont_conv.0.bias\",\n",
    "\"resnet_blocks.7.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.7.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.7.cont_conv.1.bias\",\n",
    "\"resnet_blocks.7.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.7.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.8.cont_conv.0.bias\",\n",
    "\"resnet_blocks.8.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.8.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.8.cont_conv.1.bias\",\n",
    "\"resnet_blocks.8.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.8.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.9.cont_conv.0.bias\",\n",
    "\"resnet_blocks.9.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.9.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.9.cont_conv.1.bias\",\n",
    "\"resnet_blocks.9.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.9.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.10.cont_conv.0.bias\",\n",
    "\"resnet_blocks.10.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.10.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.10.cont_conv.1.bias\",\n",
    "\"resnet_blocks.10.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.10.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.11.cont_conv.0.bias\",\n",
    "\"resnet_blocks.11.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.11.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.11.cont_conv.1.bias\",\n",
    "\"resnet_blocks.11.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.11.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.12.cont_conv.0.bias\",\n",
    "\"resnet_blocks.12.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.12.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.12.cont_conv.1.bias\",\n",
    "\"resnet_blocks.12.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.12.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.13.cont_conv.0.bias\",\n",
    "\"resnet_blocks.13.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.13.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.13.cont_conv.1.bias\",\n",
    "\"resnet_blocks.13.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.13.cont_conv.1.convolution.bias\"   ,\n",
    "\"resnet_blocks.14.cont_conv.0.bias\",\n",
    "\"resnet_blocks.14.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.14.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.14.cont_conv.1.bias\",\n",
    "\"resnet_blocks.14.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.14.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.15.cont_conv.0.bias\",\n",
    "\"resnet_blocks.15.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.15.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.15.cont_conv.1.bias\",\n",
    "\"resnet_blocks.15.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.15.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.16.cont_conv.0.bias\",\n",
    "\"resnet_blocks.16.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.16.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.16.cont_conv.1.bias\",\n",
    "\"resnet_blocks.16.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.16.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.17.cont_conv.0.bias\",\n",
    "\"resnet_blocks.17.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.17.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.17.cont_conv.1.bias\",\n",
    "\"resnet_blocks.17.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.17.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.18.cont_conv.0.bias\",\n",
    "\"resnet_blocks.18.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.18.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.18.cont_conv.1.bias\",\n",
    "\"resnet_blocks.18.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.18.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.19.cont_conv.0.bias\",\n",
    "\"resnet_blocks.19.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.19.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.19.cont_conv.1.bias\",\n",
    "\"resnet_blocks.19.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.19.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.20.cont_conv.0.bias\",\n",
    "\"resnet_blocks.20.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.20.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.20.cont_conv.1.bias\",\n",
    "\"resnet_blocks.20.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.20.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.21.cont_conv.0.bias\",\n",
    "\"resnet_blocks.21.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.21.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.21.cont_conv.1.bias\",\n",
    "\"resnet_blocks.21.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.21.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.22.cont_conv.0.bias\",\n",
    "\"resnet_blocks.22.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.22.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.22.cont_conv.1.bias\",\n",
    "\"resnet_blocks.22.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.22.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.23.cont_conv.0.bias\",\n",
    "\"resnet_blocks.23.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.23.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.23.cont_conv.1.bias\",\n",
    "\"resnet_blocks.23.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.23.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.24.cont_conv.0.bias\",\n",
    "\"resnet_blocks.24.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.24.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.24.cont_conv.1.bias\",\n",
    "\"resnet_blocks.24.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.24.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.25.cont_conv.0.bias\",\n",
    "\"resnet_blocks.25.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.25.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.25.cont_conv.1.bias\",\n",
    "\"resnet_blocks.25.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.25.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.26.cont_conv.0.bias\",\n",
    "\"resnet_blocks.26.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.26.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.26.cont_conv.1.bias\",\n",
    "\"resnet_blocks.26.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.26.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.27.cont_conv.0.bias\",\n",
    "\"resnet_blocks.27.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.27.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.27.cont_conv.1.bias\",\n",
    "\"resnet_blocks.27.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.27.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.28.cont_conv.0.bias\",\n",
    "\"resnet_blocks.28.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.28.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.28.cont_conv.1.bias\",\n",
    "\"resnet_blocks.28.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.28.cont_conv.1.convolution.bias\",\n",
    "\"resnet_blocks.29.cont_conv.0.bias\",\n",
    "\"resnet_blocks.29.cont_conv.0.convolution.weight\",\n",
    "\"resnet_blocks.29.cont_conv.0.convolution.bias\",\n",
    "\"resnet_blocks.29.cont_conv.1.bias\",\n",
    "\"resnet_blocks.29.cont_conv.1.convolution.weight\",\n",
    "\"resnet_blocks.29.cont_conv.1.convolution.bias \"\n",
    "]\n",
    "#parameter_names=[\"resnet_blocks\",\"lift,res_batch_norm_in\",\"cont_conv_layers_invariant\",\"cont_conv_layers\",\"lift\",\"project\"]\n",
    "# Group parameter names by their common prefixes\n",
    "\n",
    "layer_count_by_prefix = {}\n",
    "\n",
    "for name in parameter_names:\n",
    "    parts = name.split(\".\")[0]  # Get the part before the dot\n",
    "    count = name.split(\".\")[1]\n",
    "    if parts in layer_count_by_prefix:\n",
    "        layer_count_by_prefix[parts] += 1\n",
    "    else:\n",
    "        layer_count_by_prefix[parts] = 1\n",
    "\n",
    "print(layer_count_by_prefix)\n",
    "# Print the number of layers in each group\n",
    "for prefix, count in layer_count_by_prefix.items():\n",
    "    print(f\"Prefix: {prefix}, Number of Layers: {count}\")\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
